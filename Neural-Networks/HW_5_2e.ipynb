{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead661d-43cb-4747-bfbc-484a3d8e1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: tanh, Depth: 3, Width: 5, Validation Loss: 0.12616844475269318\n",
      "Activation: tanh, Depth: 3, Width: 10, Validation Loss: 0.0869423970580101\n",
      "Activation: tanh, Depth: 3, Width: 25, Validation Loss: 0.02375723235309124\n",
      "Activation: tanh, Depth: 3, Width: 50, Validation Loss: 0.007452488411217928\n",
      "Activation: tanh, Depth: 3, Width: 100, Validation Loss: 0.0032364290673285723\n",
      "Activation: tanh, Depth: 5, Width: 5, Validation Loss: 0.1330028772354126\n",
      "Activation: tanh, Depth: 5, Width: 10, Validation Loss: 0.0811859741806984\n",
      "Activation: tanh, Depth: 5, Width: 25, Validation Loss: 0.02092583291232586\n",
      "Activation: tanh, Depth: 5, Width: 50, Validation Loss: 0.011452448554337025\n",
      "Activation: tanh, Depth: 5, Width: 100, Validation Loss: 0.004452966619282961\n",
      "Activation: tanh, Depth: 9, Width: 5, Validation Loss: 0.2202373594045639\n",
      "Activation: tanh, Depth: 9, Width: 10, Validation Loss: 0.18241263926029205\n",
      "Activation: tanh, Depth: 9, Width: 25, Validation Loss: 0.026277920231223106\n",
      "Activation: tanh, Depth: 9, Width: 50, Validation Loss: 0.006851883139461279\n",
      "Activation: tanh, Depth: 9, Width: 100, Validation Loss: 0.004758758004754782\n",
      "Activation: relu, Depth: 3, Width: 5, Validation Loss: 0.3247015178203583\n",
      "Activation: relu, Depth: 3, Width: 10, Validation Loss: 0.16845393180847168\n",
      "Activation: relu, Depth: 3, Width: 25, Validation Loss: 0.007865634746849537\n",
      "Activation: relu, Depth: 3, Width: 50, Validation Loss: 0.0070128305815160275\n",
      "Activation: relu, Depth: 3, Width: 100, Validation Loss: 0.001015407731756568\n",
      "Activation: relu, Depth: 5, Width: 5, Validation Loss: 0.13363896310329437\n",
      "Activation: relu, Depth: 5, Width: 10, Validation Loss: 0.11135260760784149\n",
      "Activation: relu, Depth: 5, Width: 25, Validation Loss: 0.013431999832391739\n",
      "Activation: relu, Depth: 5, Width: 50, Validation Loss: 0.007767189759761095\n",
      "Activation: relu, Depth: 5, Width: 100, Validation Loss: 0.0012195692397654057\n",
      "Activation: relu, Depth: 9, Width: 5, Validation Loss: 0.29565098881721497\n",
      "Activation: relu, Depth: 9, Width: 10, Validation Loss: 0.13871556520462036\n",
      "Activation: relu, Depth: 9, Width: 25, Validation Loss: 0.027874687686562538\n",
      "Activation: relu, Depth: 9, Width: 50, Validation Loss: 0.006264244671911001\n",
      "Activation: relu, Depth: 9, Width: 100, Validation Loss: 0.0009784149006009102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values.reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# Define the neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights based on the activation function\n",
    "        if activation == 'tanh':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        elif activation == 'relu':\n",
    "            nn.init.kaiming_uniform_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc3.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x)) if self.activation == 'tanh' else F.relu(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x)) if self.activation == 'tanh' else F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # Use sigmoid for the output layer\n",
    "        return x\n",
    "\n",
    "# Load data\n",
    "train_file_path = \"C:\\\\Users\\\\santhosh\\\\Downloads\\\\project\\\\santhosh\\\\Neural-Networks\\\\dataset\\\\bank-note\\\\train.csv\"\n",
    "test_file_path = \"C:\\\\Users\\\\santhosh\\\\Downloads\\\\project\\\\santhosh\\\\Neural-Networks\\\\dataset\\\\bank-note\\\\test.csv\"\n",
    "\n",
    "X, y = load_data(train_file_path)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Specify hyperparameters\n",
    "depths = [3, 5, 9]\n",
    "widths = [5, 10, 25, 50, 100]\n",
    "activation_functions = ['tanh', 'relu']\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# Train and evaluate the neural network for each combination\n",
    "for activation in activation_functions:\n",
    "    for depth in depths:\n",
    "        for width in widths:\n",
    "            # Define and initialize the model\n",
    "            model = NeuralNetwork(input_size=X_train.shape[1], hidden_size=width, output_size=1, activation=activation)\n",
    "            \n",
    "            # Define loss and optimizer\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(epochs):\n",
    "                # Forward pass\n",
    "                outputs = model(X_train_tensor)\n",
    "                loss = criterion(outputs, y_train_tensor)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            # Evaluate on validation set\n",
    "            with torch.no_grad():\n",
    "                val_predictions = model(X_val_tensor)\n",
    "                val_loss = criterion(val_predictions, y_val_tensor)\n",
    "\n",
    "            print(f\"Activation: {activation}, Depth: {depth}, Width: {width}, Validation Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7cf73d-d648-4a2e-99b2-f608eb9dfb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
