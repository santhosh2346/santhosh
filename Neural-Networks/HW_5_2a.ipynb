{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb3c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 55.80%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    X = data.iloc[:, :-1].values  # Features (all columns except the last one)\n",
    "    y = data.iloc[:, -1].values   # Labels (last column)\n",
    "    y = y.reshape(-1, 1)  # Reshape labels for compatibility\n",
    "    return X, y\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def initialize_weights(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    # Initialize weights randomly\n",
    "    weights = {\n",
    "        'W1': np.random.rand(input_size, hidden1_size),\n",
    "        'W2': np.random.rand(hidden1_size, hidden2_size),\n",
    "        'W3': np.random.rand(hidden2_size, output_size)\n",
    "    }\n",
    "    return weights\n",
    "\n",
    "def forward_propagation(X, weights):\n",
    "    # Input to hidden layer 1\n",
    "    z1 = np.dot(X, weights['W1'])\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    # Hidden layer 1 to hidden layer 2\n",
    "    z2 = np.dot(a1, weights['W2'])\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hidden layer 2 to output layer\n",
    "    z3 = np.dot(a2, weights['W3'])\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a1, a2, a3\n",
    "\n",
    "def backward_propagation(X, y, a1, a2, a3, weights, learning_rate):\n",
    "    # Compute gradients\n",
    "    error = a3 - y\n",
    "    delta3 = error * sigmoid_derivative(a3)\n",
    "    dW3 = np.outer(a2, delta3)  # Use outer product to match dimensions\n",
    "\n",
    "    delta2 = np.dot(delta3, weights['W3'].T) * sigmoid_derivative(a2)\n",
    "    dW2 = np.outer(a1, delta2)\n",
    "\n",
    "    delta1 = np.dot(delta2, weights['W2'].T) * sigmoid_derivative(a1)\n",
    "    dW1 = np.outer(X, delta1)\n",
    "\n",
    "    # Update weights\n",
    "    weights['W1'] -= learning_rate * dW1\n",
    "    weights['W2'] -= learning_rate * dW2\n",
    "    weights['W3'] -= learning_rate * dW3\n",
    "\n",
    "    return weights\n",
    "\n",
    "def train_neural_network_with_early_stopping(X_train, y_train, input_size, hidden1_size, hidden2_size, output_size, epochs, learning_rate, patience=10):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    weights = initialize_weights(input_size, hidden1_size, hidden2_size, output_size)\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_weights = weights\n",
    "    count = 0  # Counter for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X_train)):\n",
    "            # Forward propagation\n",
    "            a1, a2, a3 = forward_propagation(X_train[i], weights)\n",
    "\n",
    "            # Backward propagation\n",
    "            weights = backward_propagation(X_train[i], y_train[i], a1, a2, a3, weights, learning_rate)\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        val_predictions = predict(X_val, weights)\n",
    "        val_accuracy = np.mean((val_predictions > 0.5) == y_val)\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_weights = weights\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if count >= patience:\n",
    "            break\n",
    "\n",
    "    return best_weights\n",
    "\n",
    "def predict(X, weights):\n",
    "    _, _, predictions = forward_propagation(X, weights)\n",
    "    return predictions\n",
    "\n",
    "# Load training data\n",
    "\n",
    "train_file_path = \"C:\\\\Users\\\\santhosh\\\\Downloads\\\\project\\\\santhosh\\\\Neural-Networks\\\\dataset\\\\bank-note\\\\train.csv\"\n",
    "test_file_path = \"C:\\\\Users\\\\santhosh\\\\Downloads\\\\project\\\\santhosh\\\\Neural-Networks\\\\dataset\\\\bank-note\\\\test.csv\"\n",
    "\n",
    "\n",
    "X_train, y_train = load_data(train_file_path)\n",
    "X_test, y_test = load_data(test_file_path)\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are the training and test data and labels\n",
    "input_size = X_train.shape[1]\n",
    "hidden1_size = 4  # Adjust as needed\n",
    "hidden2_size = 3  \n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "patience = 20  \n",
    "\n",
    "# Train the neural network with early stopping\n",
    "best_weights = train_neural_network_with_early_stopping(X_train, y_train, input_size, hidden1_size, hidden2_size, output_size, epochs, learning_rate, patience)\n",
    "\n",
    "# Test the neural network on the test data\n",
    "predictions = predict(X_test, best_weights)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = np.mean((predictions > 0.5) == y_test)\n",
    "print(f\"Accuracy on test data: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc29a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
